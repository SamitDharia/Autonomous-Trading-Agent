Project Brief (print this—this is the source of truth)

Goal (single sentence):
Build a reliable, low-maintenance trading bot that predicts whether the next 60 minutes will move enough (after costs) to justify a trade, sizes positions by confidence, and obeys strict risk limits.

Rails:

Platform: QuantConnect (LEAN Cloud)

Brokerage: Alpaca (paper first)

Code: Python (single QC project to keep it simple)

Storage: QC Object Store for models

Universe & horizon (start narrow):

Symbols: TSLA only (Week 1–2), then add a small basket (e.g., AAPL, MSFT, SPY)

Bars: 5-minute bars

Prediction horizon: next 60 minutes (12 bars)

Features (inputs):

Price/volume bars (OHLCV)

Indicators: RSI(14), EMA(20/50/200) + slopes, MACD(12,26,9) + hist, ATR(14), Bollinger z-score(20,2)

Regime: realized volatility last 24 bars, time-of-day, day-of-week

Models (stacked ensemble):

Level-1 experts (tiny, robust):

RSI expert → probability of positive 60-min return using RSI + z-score + slope

MACD expert → using MACD, signal, histogram, slope

Trend expert → using EMA(20/50/200), crossovers, slopes

(Add a Volatility expert later)

Level-2 brain (meta-model): logistic regression (start) combining expert probabilities + 2 regime features (volatility, time-of-day). Output is final probability p ∈ [0,1].

Decision & sizing:

Trade only if |p − 0.5| ≥ 0.05 (minimum edge)

Position size: volatility-scaled (ATR-based), capped at 1% of equity per position

Orders: bracket (stop-loss and take-profit); min hold 15 min to reduce churn

Risk (hard stops):

Daily stop: −1% of equity → go flat, stop trading for the day

Max concurrent positions: 1 (Week 1–2), later 3

Kill-switch: any data/model/broker error → flatten and pause

Backtesting:

Include costs: fees + realistic spread/slippage

Walk-forward: train on past N months, validate on next month, roll forward

Promote strategies only if they beat baselines (do-nothing & MA crossover) after costs

Rollout:

Backtest → Paper (1–2 weeks) → tiny Live only after stability

One-button rollback to last good version

Repo / Project layout (conceptual; QC can keep most in one file)
/docs/
  PROJECT_BRIEF.md
  NOTES.md                    # running diary: decisions, why, results
/features/
  feature_builder.py          # build indicator & regime features
/experts/
  rsi_expert.py               # load + predict_proba(features)
  macd_expert.py
  trend_expert.py
/ensemble/
  brain.py                    # load brain model; blend expert outputs + regime
/risk/
  position_sizing.py          # prob->size (ATR scale, caps)
  guards.py                   # daily stop, kill-switch checks
/models/
  rsi_expert.json             # saved tiny models (or .pkl)
  macd_expert.json
  trend_expert.json
  brain.pkl
/tests/
  test_features.py
  test_rsi_expert.py
  test_position_sizing.py
algo.py                       # QuantConnect main algorithm (wires it all)


On QuantConnect you can keep helpers inside algo.py to start. Upload the model files to QC Object Store and load them in Initialize().

Roadmap with exact steps (copy this checklist)
Phase 0 — Prep (1–2 hours)

Copy the Project Brief above into /docs/PROJECT_BRIEF.md.

Create a new QuantConnect Python project (cloud).

In Alpaca, confirm paper API keys are set in QC (brokerage settings).

In QC, create a blank algorithm file algo.py (leave the default template).

Phase 1 — Skeleton that runs end-to-end (0.5–1 day)

Goal: A bot that computes indicators and can place dummy bracket orders (e.g., RSI rule), with risk guards—but no models yet.
Steps:

Compute RSI, EMA(20/50/200), MACD, ATR, Bollinger in Initialize() and warm them.

In OnData, build a small features dict (RSI value, MACD hist, ATR %, etc.).

Implement dummy rule:

If RSI < 30 → long; if RSI > 70 → flat.

Use position_sizing: fixed 0.5% equity; bracket orders using ATR.

Add guards: stop trading if daily P&L <= −1% (paper metric in QC), and if any indicator isn’t ready.

Backtest 6–12 months. Confirm it runs start-to-finish with logs.

Phase 2 — Experts (3–4 days)

Goal: Train three tiny experts offline; load and call them in QC.
Offline steps (laptop or notebook):

Export historical bar data for TSLA (QC has data; you can also sample locally).

Build features: the indicators above + regime (volatility, time-of-day).

Label: future 60-min return > (fees + slip)? → 1 else 0.

Train tiny models per expert (logistic or XGBoost depth≤3).

Calibrate outputs (Platt or isotonic) on validation folds.

Save rsi_expert.json, macd_expert.json, trend_expert.json.
QC steps:

Upload model files to Object Store.

In Initialize(), load models.

In OnData, build features → call predict_proba() on each expert → log their probabilities.

Phase 3 — Brain + Sizing (2–3 days)

Goal: Blend expert probs with a meta-model; replace dummy rule with prob→size.
Offline:

Using validation data, collect expert outputs + regime → train brain (logistic regression).

Calibrate brain probabilities; save brain.pkl.
QC:

Load brain.pkl in Initialize().

In OnData, compute p = brain.predict_proba([...]).

Map p to position with ATR scaling and caps (1% of equity max).

Place bracket orders; add min hold time 15 min.

Backtest with costs; log all key metrics.

Phase 4 — Paper & stability (1–2 weeks)

Goal: Zero drama. Clean logs. No churn. Edges survive costs.
Steps:

Paper trade TSLA only for a week.

Watch: turnover, slippage, drawdowns vs daily stop, how often guards trip.

Adjust thresholds (e.g., edge threshold from 0.05 → 0.08) to reduce churn.

Phase 5 — Extend cautiously (Week 3+)

Add 2–4 symbols.

Add Volatility expert; re-train brain.

Introduce Champion vs Challenger: keep current logic as “champion,” run a slightly different challenger in shadow (paper). Promote only if it beats the champion after costs.

Phase 6 — Tiny live (Week 4+)

Go live with micro size (e.g., €100 notional caps per trade).

Keep daily stop tight.

Weekly review; scale only after 4 calm, profitable weeks.

Prompt Library (copy-paste exactly; swap details where needed)
A) Architecture & plan (Claude or Gemini; they handle long context well)

You are my system architect. Using the brief below, design a minimal, robust stacked-ensemble trading system for QuantConnect (LEAN Cloud) with Alpaca paper.
Deliver: (1) file layout, (2) feature flow, (3) data labeling & walk-forward plan with purging/embargo, (4) model training + calibration steps, (5) risk controls, (6) backtest→paper→live checklist.
Keep it boring and testable.
BRIEF: [paste /docs/PROJECT_BRIEF.md]

B) QC skeleton (GPT-5 / Copilot Chat in VS Code or QC editor)

Generate a unified diff that only updates algo.py to:

Add RSI(14), EMA(20/50/200), MACD(12,26,9), ATR(14), Bollinger(20,2) with proper warm-up.

Build a features dict in OnData.

Implement a dummy RSI rule (RSI<30 → long; RSI>70 → flat) using bracket orders sized at 0.5% equity and ATR-based stops.

Add daily P&L stop of −1% and a 15-min minimum hold.
No other files. Show only the diff.

C) Expert loader (GPT-5)

Show a diff that adds experts/rsi_expert.py and imports in algo.py.
rsi_expert.py must provide load(obj_store, key) and predict_proba(features: dict) -> float using a pre-trained, calibrated logistic or tiny XGBoost model.
In algo.py Initialize(), load the model from Object Store and store a handle. In OnData, call it and log the probability.
Only change these files. Show the diff.

D) Tests first (GPT-5)

Write tests in tests/test_position_sizing.py and tests/test_rsi_expert.py:

size_from_prob(p, atr_pct, cap) returns 0 at p=0.5, increases toward +cap as p→1, decreases toward −cap as p→0; scales inversely with ATR.

rsi_expert predictions are within [0,1] and are monotone with RSI z-score on a tiny synthetic dataset.
Output only the diffs.

E) Brain wiring (GPT-5)

Add ensemble/brain.py with load(obj_store, key) and predict_proba(expert_probs: dict, regime: dict).
Modify algo.py to use expert probabilities + regime to compute final p, then map to size with risk/position_sizing.py, place bracket orders.
Only those files. Show the diff.

F) Offline training notebook outline (any model)

Outline a step-by-step notebook to:

Load bars,

Build features & labels (next 60-min return > fees+slip → 1 else 0),

Purge & embargo splits,

Train 3 tiny experts (logistic or XGB depth ≤3),

Calibrate with Platt or isotonic,

Generate out-of-fold expert probs,

Train/calibrate the brain on expert probs + regime,

Save models to files compatible with QC loading.
Output: a bulleted plan and minimal code snippets.

G) Bug triage prompt (when tests fail)

Here is a failing test and stack trace. Explain the root cause in one paragraph and propose the smallest diff to fix it. Then show only that diff.
[paste failure]

Operational guardrails (do these and you won’t keep re-breaking things)

One change per request. Each AI prompt should modify exactly one concern (e.g., add RSI expert).

Ask for diffs only. Never ask the AI to “rewrite the file.”

Check in after every success. Commit with message “checkpoint: runs end-to-end.”

Paper first, always. Keep live off until a full quiet week of paper stability.

Keep a diary. In /docs/NOTES.md add date → change → result. This prevents context loss.

Backtest standards (pass/fail gates)

A backtest is “green-light” only if all of these are true (after costs):

Beats do-nothing and a simple MA crossover baseline.

Max drawdown within your tolerance (e.g., < 6–8% in test).

Turnover reasonable (avoid constant in/out).

Slippage assumptions realistic (don’t cheat).

Stable across regimes (doesn’t only work on one month).

Troubleshooting (fast triage)

Bot won’t start → Indicators not warmed / model not found → add warm-up / check Object Store keys.

Trades way too often → raise edge threshold; add min hold time; add hysteresis (only act when p changes by >0.05).

Backtest great, paper bad → slippage too low in backtest; tighten costs; reduce size; verify order type and market hours.

Daily stop triggering a lot → lower cap per trade (0.5%); increase threshold; pause during high-vol news hours.